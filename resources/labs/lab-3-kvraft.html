<!DOCTYPE html>
<!-- saved from url=(0053)https://pdos.csail.mit.edu/6.824/labs/lab-kvraft.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<link rel="stylesheet" href="./style.css" type="text/css">
<title>6.824 Lab 3: Fault-tolerant Key/Value Service</title>
</head>
<body>
<div align="center">
<h2><a href="https://pdos.csail.mit.edu/6.824/index.html">6.824</a> - Spring 2017</h2>
<h1>6.824 Lab 3: Fault-tolerant Key/Value Service</h1>
<h3>Due Part A: Fri Mar 17 11:59pm</h3>
<h3>Due Part B: Fri Apr 14 11:59pm</h3>
</div>
<hr>

<h3>Introduction</h3>

<p>
In this lab you will build a fault-tolerant key-value storage
service using your Raft library from
<a href="https://pdos.csail.mit.edu/6.824/labs/lab-raft.html">lab 2</a>. You will build your
key-value service as a replicated state machine, consisting of
several key-value servers that coordinate their activities
through the Raft log. Your key/value service should continue to
process client requests as long as a majority of the servers
are alive and can communicate, in spite of other failures or
network partitions.

</p><p>
Your system will consist of clients and key/value servers,
where each key/value server also acts as a Raft peer. Clients
send <tt>Put()</tt>, <tt>Append()</tt>, and <tt>Get()</tt> RPCs
to key/value servers (called kvraft servers), which then place
those calls into the Raft log and execute them in order. A
client can send an RPC to any of the kvraft servers, but should
retry by sending to a different server if the server is not
currently a Raft leader, or if there's a failure. If the
operation is committed to the Raft log (and hence applied to
the key/value state machine), its result is reported to the
client. If the operation failed to commit (for example, if the
leader was replaced), the server reports an error, and the
client retries with a different server.

</p><p>
This lab has two parts. In part A, you will implement the
service without worrying that the Raft log can grow without
bound. In part B, you will implement snapshots (Section 7 in
the paper), which will allow Raft to garbage collect old log
entries. Please submit each part by the respective deadline.

</p><ul class="hints">
<li>
This lab doesn't require you to write much code, but
you will most likely spend a substantial amount of time
thinking and staring at debugging logs to figure out
why your implementation doesn't work. Debugging will be
more challenging than in the Raft lab because there are
more components that work asynchronously of each other.
Start early.

</li><li>
You should reread the
<a href="https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf">extended Raft paper</a>,
in particular Sections 7 and 8. For a wider
perspective, have a look at Chubby, Raft Made Live,
Spanner, Zookeeper, Harp, Viewstamped Replication, and
<a href="http://static.usenix.org/event/nsdi11/tech/full_papers/Bolosky.pdf">Bolosky et al.</a>
</li></ul>

<h3>Collaboration Policy</h3>

You must write all the code you hand in for 6.824, except for
code that we give you as part of the assignment. You are not
allowed to look at anyone else's solution, you are not allowed
to look at code from previous years, and you are not allowed to
look at other Raft implementations. You may discuss the
assignments with other students, but you may not look at or
copy each others' code. Please do not publish your code or make
it available to future 6.824 students -- for example, please do
not make your code visible on GitHub (instead, create a private
repository on MIT's <a href="https://github.mit.edu/">GitHub
deployment</a>).

<h3>Getting Started</h3>

<p>
Do a <tt>git pull</tt> to get the latest lab software. We
supply you with skeleton code and tests in
<tt>src/kvraft</tt>. You will need to modify
<tt>kvraft/client.go</tt>, <tt>kvraft/server.go</tt>, and
perhaps <tt>kvraft/common.go</tt>.

</p><p>
To get up and running, execute the following commands:
</p><pre>$ cd ~/6.824
$ git pull
...
$ cd src/kvraft
$ GOPATH=~/6.824
$ export GOPATH
$ go test
...
$</pre>

When you're done, your implementation should pass all the tests
in the <tt>src/kvraft</tt> directory:

<pre>$ go test
Test: One client ...
... Passed
Test: concurrent clients ...
... Passed
Test: unreliable ...
... Passed
...
PASS
ok  	kvraft	345.032s</pre>


<h3>Part A: Key/value service without log compaction</h3>
<p>
The service supports three RPCs: <tt>Put(key, value)</tt>,
<tt>Append(key, arg)</tt>, and <tt>Get(key)</tt>. It maintains
a simple database of key/value pairs. <tt>Put()</tt> replaces
the value for a particular key in the database, <tt>Append(key,
arg)</tt> appends arg to key's value, and <tt>Get()</tt>
fetches the current value for a key. An <tt>Append</tt> to
a non-existant key should act like <tt>Put</tt>.

</p><p>
You will implement the service as a replicated state machine
consisting of several kvservers. Your kvraft client code
(<tt>Clerk</tt> in <tt>src/kvraft/client.go</tt>) should try
different kvservers it knows about until one responds
positively. As long as a client can contact a kvraft server
that is a Raft leader in a majority partition, its operations
should eventually succeed.

</p><div class="todo">
<p>
Your first task is to implement a solution that works when there are no dropped
messages, and no failed servers. Your service must ensure
that <tt>Get()</tt>, <tt>Put()</tt>, and <tt>Append</tt> return results that
are <a href="https://en.wikipedia.org/wiki/Linearizability"><em>linearizable</em></a>. That
is, completed application calls to the <tt>Clerk.Get()</tt>,
<tt>Clerk.Put()</tt>, and <tt>Clerk.Append()</tt>
methods in <tt>kvraft/client.go</tt> must appear to
all clients to have affected the service in the same linear order,
even in there are failures and leader changes.
A <tt>Clerk.Get(key)</tt> that starts
after a completed <tt>Clerk.Put(key, …)</tt>
or <tt>Clerk.Append(key, …)</tt> 
should see the value written by the most recent
<tt>Clerk.Put(key, …)</tt> or
<tt>Clerk.Append(key, …)</tt> in the linear
order.
Completed calls should have exactly-once semantics.
</p><p>
A reasonable plan of attack may be to first fill in the
<tt>Op</tt> struct in <tt>server.go</tt> with the
"value" information that kvraft will use Raft to agree
on (remember that <tt>Op</tt>
field names must start with capital letters, since they
will be sent through RPC), and then implement the
<tt>PutAppend()</tt> and <tt>Get()</tt> handlers in
<tt>server.go</tt>. The handlers should enter an
<tt>Op</tt> in the Raft log using <tt>Start()</tt>, and
should reply to the client when that log entry is committed.
Note that you <strong>cannot</strong> execute
an operation until the point at which it is committed in
the log (i.e., when it arrives on the Raft
<tt>applyCh</tt>).

</p><p>
You have completed this task when you
<strong>reliably</strong> pass the first test in the
test suite: "One client". You may also find that you
can pass the "concurrent clients" test, depending on
how sophisticated your implementation is.
</p></div>

<p class="note">
Your kvraft servers should not directly communicate; they
should only interact with each other through the Raft log.

</p><ul class="hints">
<li>
After calling <tt>Start()</tt>, your kvraft
servers will need to wait for Raft to complete
agreement. Commands that have been agreed upon arrive
on the <tt>applyCh</tt>. You should think carefully
about how to arrange your code so that it will
keep reading <tt>applyCh</tt>, while
<tt>PutAppend()</tt> and <tt>Get()</tt> handlers submit
commands to the Raft log using <tt>Start()</tt>. It is
easy to achieve deadlock between the kvserver and its
Raft library.

</li><li>
Your solution needs to handle the case in
which a leader has called Start() for a client
RPC, but loses its leadership before the
request is committed to the log. In this case
you should arrange for the client to re-send
the request to other servers until it finds
the new leader. One way to do this is for the
server to detect that it has lost leadership,
by noticing that a different request has
appeared at the index returned by Start(), or
that the term reported by Raft.GetState() has
changed. If the ex-leader is partitioned by
itself, it won't know about new leaders; but
any client in the same partition won't be able
to talk to a new leader either, so it's OK in
this case for the server and client to wait
indefinitely until the partition heals.

</li><li>
You will probably have to modify your client
Clerk to remember which server turned out to
be the leader for the last RPC, and send the
next RPC to that server first. This will avoid
wasting time searching for the leader on every
RPC, which may help you pass some of the tests
quickly enough.

</li><li>
A kvraft server should not complete a <tt>Get()</tt>
RPC if it is not part of a majority (so that it does
not serve stale data). A simple solution is to enter
every <tt>Get()</tt> (as well as each <tt>Put()</tt>
and <tt>Append()</tt>) in the Raft log. You don't have
to implement the optimization for read-only operations
that is described in Section 8.

</li><li>
It's best to add locking relatively early in development because the
need to avoid deadlocks sometimes affects overall code design. Check
that your code is race-free using
<tt>go test -race</tt>.

</li></ul>

In the face of unreliable connections and server failures, a
client may send an RPC multiple times until it finds a kvraft
server that replies positively. If a leader fails just after
committing an entry to the Raft log, the client may believe
that the request failed, and re-send it to another leader.
Each call to
<tt>Clerk.Put()</tt> or <tt>Clerk.Append()</tt> should
result in just a single execution, so you will have to ensure
that the re-send doesn't result in the servers executing
the request twice.

<p class="todo">
Add code to cope with duplicate client requests, including
situations where the client sends a request to a kvraft leader
in one term, times out waiting for a reply, and re-sends the
request to a new leader in another term. The client request
should always execute just once. To pass part A, your service
should <strong>reliably</strong> pass all tests through
<tt>TestPersistPartitionUnreliable()</tt>.

</p><ul class="hints">
<li>
You will need to uniquely identify client operations to ensure that
the key/value service executes each one just once.

</li><li>
Your scheme for duplicate detection should free server memory quickly,
for example by having each RPC imply that the client has seen the
reply for its previous RPC. It's OK to assume that a client will make
only one call into a clerk at a time.

</li></ul>

<h3>Handin procedure for lab 3A</h3>

<div class="important">
<p>
Before submitting, please run the tests for part A
one final time.
Some bugs may not appear on
every run, so run the tests multiple times.
</p></div>

<p>
Submit your code via the class's submission website, located at
<a href="https://6824.scripts.mit.edu/2017/handin.py/">https://6824.scripts.mit.edu/2017/handin.py/</a>.

</p><p>
You may use your MIT Certificate or request an API key via
email to log in for the first time. Your API key (<tt>XXX</tt>)
is displayed once you are logged in, which can be used to upload
the lab from the console as follows.

</p><pre>$ cd "$GOPATH"
$ echo "XXX" &gt; api.key
$ make lab3a</pre>

<p class="important">
Check the submission website to make sure you submitted a working lab!

</p><p class="note">
You may submit multiple times. We will use the timestamp of
your <strong>last</strong> submission for the purpose of
calculating late days. Your grade is determined by the score
your solution <strong>reliably</strong> achieves when we run
the tester on our test machines.

</p><h3>Part B: Key/value service with log compaction</h3>

<p>
As things stand now with your lab code, a rebooting server replays the
complete Raft log in order to restore its state. However, it's not
practical for a long-running server to remember the complete Raft log
forever. Instead, you'll modify Raft and kvraft to cooperate to save
space: from time to time kvraft will persistently store a "snapshot"
of its current state, and Raft will discard log entries that precede
the snapshot. When a server restarts (or falls far behind the leader
and must catch up), the server first installs a snapshot and then
replays log entries from after the point at which the snapshot
was created.
Section 7 of the
<a href="https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf">extended Raft paper</a>
outlines the scheme; you will have to design the details.

</p><p>
You should spend some time figuring out what the
interface will be between your Raft library and your
service so that your Raft library can discard log
entries. Think about how your Raft will operate while
storing only the tail of the log, and how it will
discard old log entries. You should discard them in a
way that allows the Go garbage collector to free and
re-use the memory; this requires that there be no
reachable references (pointers) to the discarded log
entries.

</p><p>
The kvraft tester passes <tt>maxraftstate</tt> to your
<tt>StartKVServer()</tt>. <tt>maxraftstate</tt> indicates the maximum
allowed size of your persistent Raft state in bytes (including the
log, but not including snapshots). You should
compare <tt>maxraftstate</tt> to <tt>persister.RaftStateSize()</tt>.
Whenever your key/value server detects that the Raft state size is
approaching this threshold, it should save a snapshot, and tell the
Raft library that it has snapshotted, so that Raft can discard old log
entries.

</p><p class="todo">
Your <tt>raft.go</tt> probably keeps the entire log in a Go slice.
Modify it so that it can be given a log index, discard the entries
before that index, and continue operating while storing only log
entries after that index. Make sure you pass all the Raft tests after
making these changes.

</p><p class="todo">
Modify your kvraft server so that it detects when the persisted
Raft state grows too large, and then saves a snapshot and tells
Raft that it can discard old log entries.
Save each snapshot with persister.SaveSnapshot()
(don't use files).

</p><p class="todo">
Modify your Raft leader code to send an InstallSnapshot
RPC to a follower when the leader has discarded the log
entries the follower needs.
When a follower receives an InstallSnapshot RPC,
your Raft code will need to send the included snapshot to
its kvraft. You can use the <tt>applyCh</tt>
for this purpose — see the <tt>UseSnapshot</tt>
field.
A kvraft instance should restore the snapshot from the
persister when it re-starts.
Your
solution is complete when you pass the remaining tests
<strong>reliably</strong>.

</p><p class="note">
The <tt>maxraftstate</tt> limit applies to the GOB-encoded
bytes your Raft passes to <tt>persister.SaveRaftState()</tt>.

</p><ul class="hints">
<li>
Think about when a kvserver should snapshot its state
and what should be included in the snapshot. You must
store each snapshot in the persister object using
<tt>SaveSnapshot()</tt>,
<strong>not</strong> along with the other Raft state in
<tt>SaveRaftState()</tt>. You can read the
latest stored snapshot using <tt>ReadSnapshot()</tt>.

</li><li>
Remember that your kvserver must be able to detect
duplicate client requests across checkpoints, so any
state you are using to detect them must be included in
the snapshots. Remember to capitalize all fields of
structures stored in the snapshot.

</li><li>
Make sure you pass <tt>TestSnapshotRPC</tt> before
moving on to the other Snapshot tests.

</li><li>
A common source of bugs is for the Raft state and the
snapshot state to be inconsistent with each other, particularly
after re-starts or InstallSnapshots.
You are allowed to add methods to your Raft for
kvserver to call to help handle InstallSnapshot RPCs.

</li></ul>

<h3>Handin procedure for lab 3B</h3>

<div class="important">
<p>
Before submitting, please run <em>all</em> the tests
one final time, including the Raft tests.

</p><pre>$ go test</pre>
</div>

<p>
Submit your code via the class's submission website, located at
<a href="https://6824.scripts.mit.edu/2017/handin.py/">https://6824.scripts.mit.edu/2017/handin.py/</a>.

</p><p>
You may use your MIT Certificate or request an API key via
email to log in for the first time. Your API key (<tt>XXX</tt>)
is displayed once you logged in, which can be used to upload
the lab from the console as follows.

</p><pre>$ cd "$GOPATH"
$ echo "XXX" &gt; api.key
$ make lab3b</pre>

<p class="important">
Check the submission website to make sure you submitted a working lab!

</p><p class="note">
You may submit multiple times. We will use the timestamp of
your <strong>last</strong> submission for the purpose of
calculating late days. Your grade is determined by the score
your solution <strong>reliably</strong> achieves when we run
the tester on our test machines.

</p><hr>

<address>
Please post questions on <a href="http://piazza.com/">Piazza</a>.
</address>



<!--  LocalWords:  RPCs viewservice src pbservice cd view's PingInterval ack pb
-->
<!--  LocalWords:  DeadPings Viewservice ViewServer PingArgs Viewnum viewservice
-->
<!--  LocalWords:  Handin gzipped czvf whoami tgz GOPATH TestBasicFail GetReply
-->
<!--  LocalWords:  TestFailPut TestConcurrentSame TestPartition PutReply Raft
-->
<!--  LocalWords:  kvraft instance's Viewstamped al paxos TestBasic seq Go's
-->
<!--  LocalWords:  ndecided TestForget px int bool ok Min Raft's piggyback un
-->
<!--  LocalWords:  struct Op IDs pm structs marshall unmarshall class's
-->
<!--  LocalWords:  StartServer kvrafts website API
-->
</body></html>
